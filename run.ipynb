{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "657efdbf-68ff-4ae4-8845-f47ea550a49c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Project 1 of Machine Learning CS-433, 2023\n",
    "\n",
    "### _By_ _Salya_ _Diallo_, _Shrinidi_ _Singaravelan_ _and_ _Fanny_ _Ghez_\n",
    "\n",
    "In this project, our main goal is to determine the risk of a person in developing CVD (Cardiovascular Diseases) based on features of their personal lifestyle factors, using the given data set.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e92fa511-bb9d-4991-95be-9e026d5bc53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2e9b245-a466-4a33-8b7d-368163f5015d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from helpers import *\n",
    "from standardization import *\n",
    "from clean_and_predict import *\n",
    "from implementations import *\n",
    "from cross_validation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529497b8-9be8-4ddf-864e-7d0937e1afe0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### We then import and clean our train and set data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ce9f2ba-3635-4eb6-9ca0-051cbca7b411",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You have to change the path in function of where the data set is located in your computer\n",
    "x_train, x_test, y_train1, train_ids, test_ids = load_csv_data('dataset_to_release', sub_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266b634b-25e0-4656-97d1-53ac1b581f40",
   "metadata": {},
   "source": [
    "#### We do not want any _NaN_ values in our data. Especially in x_train and x_test there is unknown numerical values: we want to change them into another value, for example the median of the corresponding column, which is what we will do now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b135c23-9ef4-4452-a472-8df0ab59d675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Youpi!\n",
      "(138783, 322)\n",
      "Youpi!\n",
      "(189352, 322)\n"
     ]
    }
   ],
   "source": [
    "x_train1, i_1, mean_1, std_1 = clean_data(x_train, [1])\n",
    "x_train2, i_2, mean_2, std_2 = clean_data(x_train, [2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ca78320-a506-483f-bbc1-5cfff47153bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Youpi!\n",
      "(46401, 322)\n",
      "Youpi!\n",
      "(62978, 322)\n"
     ]
    }
   ],
   "source": [
    "# We need to create another clean function -> because our standardize function will not be the same\n",
    "# We will be using the mean and std of our train set:\n",
    "\n",
    "x_test1, ind_1_test = clean_data_test_set(x_test, [1], mean_1, std_1) \n",
    "x_test2, ind_2_test = clean_data_test_set(x_test, [2], mean_2, std_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4079893f-5e41-4fbb-958a-25be4de3d4f1",
   "metadata": {},
   "source": [
    "#### For y_train, there is non _NaN_ values but only -1 or 1. Or, we want to have 0s instead os -1s. Hence we will only modify this for y_train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fedb285a-8aaa-4c19-a722-1572cbc0a530",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train = y_train1.copy()\n",
    "y_train = np.where(y_train == -1, 0, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6b0a2ab-7b76-4a08-bfd6-a3e932f5f424",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(328135,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a576dfc-a43e-4e8d-bb59-051cc206eac5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### We now create a function that will compute the mistake percentage of our prediction of y_train, and then our prediction for y_test, in function of the weights obtained from one of our algorithm. This will return the prediction and print the mistake percentage and the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f773b7a-d87e-4c09-8704-b25929b60be1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def error_and_prediction(w_1, w_2):\n",
    "    # Let us first compute and print the mistake percentage of our prediction of y_train:\n",
    "    y_pred_1 = predict_labels(w_1, x_train1)\n",
    "    y_pred_2 = predict_labels(w_2, x_train2)\n",
    "    \n",
    "    e1 = np.count_nonzero(y_pred_1 - y_train[i_1])\n",
    "    e2 = np.count_nonzero(y_pred_2 - y_train[i_2])\n",
    "\n",
    "    print('The mistake percentage on the train set is:',round((e1+e2)*100/y_train.shape[0], 4), '%.')\n",
    "    \n",
    "    # Now we predict and print our y_test:\n",
    "    y_pred_test_1 = predict_labels(w_1, x_test1)\n",
    "    y_pred_test_2 = predict_labels(w_2, x_test2)\n",
    "    \n",
    "    prediction = np.zeros(len(test_ids))\n",
    "    \n",
    "    prediction[ind_1_test] = y_pred_test_1\n",
    "    prediction[ind_2_test] = y_pred_test_2\n",
    "    # We still need to change the zeros into -1s since y_train initially have -1 instead of zeros\n",
    "    prediction[prediction==0] = -1\n",
    "    print('Our prediction for y_test is:', prediction,'.')\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b137506-7f86-423a-b5a0-458ea9d22c90",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Gradient Descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71225e22-ea93-442b-afdd-471c41f4fd06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialization:\n",
    "D1 = x_train1.shape[1]\n",
    "initial_w1 = np.zeros((D1,))\n",
    "\n",
    "# Parameters: \n",
    "gamma = 0.001\n",
    "max_iters = 100\n",
    "\n",
    "# Computation of the weights and loss:\n",
    "w_gd1, loss_gd1 = mean_squared_error_gd(y_train[i_1], x_train1, initial_w1, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8481d025-521c-4792-a4b7-1d1035c54edd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialization:\n",
    "D2 = x_train2.shape[1]\n",
    "initial_w2 = np.zeros((D2,))\n",
    "\n",
    "# Parameters: \n",
    "gamma = 0.001\n",
    "max_iters = 100\n",
    "\n",
    "# Computation of the weights and loss:\n",
    "w_gd2, loss_gd2 = mean_squared_error_gd(y_train[i_2], x_train2, initial_w2, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40237a5c-7012-44ca-85f9-fdbb2e52319a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04901843315569615\n",
      "0.03233364855262832\n"
     ]
    }
   ],
   "source": [
    "print(loss_gd1)\n",
    "print(loss_gd2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46299e68-6ccf-4761-b74d-e923ba203ef1",
   "metadata": {},
   "source": [
    "### We now want to predict our y. To do so we use our previously defined function to compute the error percentage and our prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35d76407-a8ab-48f6-bf2b-ab439dcfe3a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mistake percentage on the train set is: 8.8262 %.\n",
      "Our prediction for y_test is: [-1. -1. -1. ... -1. -1. -1.] .\n"
     ]
    }
   ],
   "source": [
    "y_pred_gd = error_and_prediction(w_gd1, w_gd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e2f5563-9b17-4ec5-a6e9-3cd486ba9958",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'Gradient_Descent' \n",
    "create_csv_submission(test_ids, y_pred_gd, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455a7cdd-af05-440c-b772-d9458c68dbc7",
   "metadata": {},
   "source": [
    "F1 score: 0.411\n",
    "\n",
    "Accuracy: 0.843"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2af7b1d-cb71-4e43-85bb-9bbada09abfe",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea1913c4-c7c7-4712-8d94-e6d36cb0333a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialization:\n",
    "w1 = np.zeros((x_train1.shape[1],))\n",
    "\n",
    "# Parameters: \n",
    "gamma = 0.001\n",
    "max_iters = 100\n",
    "\n",
    "# Computation of the weights and loss:\n",
    "w_sgd1, loss_sgd1 = mean_squared_error_sgd(y_train[i_1], x_train1, w1, 1, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c562859a-fd4a-4651-a891-96d7794f7e5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialization:\n",
    "w2 = np.zeros((x_train2.shape[1],))\n",
    "\n",
    "# Parameters: \n",
    "gamma = 0.001\n",
    "max_iters = 100\n",
    "\n",
    "# Computation of the weights and loss:\n",
    "w_sgd2, loss_sgd2 = mean_squared_error_sgd(y_train[i_2], x_train2, w2, 1, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1821278a-7d01-4122-9b70-5f4128068f35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05203075128358397\n",
      "0.03334871416215792\n"
     ]
    }
   ],
   "source": [
    "print(loss_sgd1)\n",
    "print(loss_sgd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14d5fad2-79e6-4a86-b824-bb6bc4c360b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mistake percentage on the train set is: 9.5445 %.\n",
      "Our prediction for y_test is: [-1. -1. -1. ... -1.  1. -1.] .\n"
     ]
    }
   ],
   "source": [
    "y_pred_sgd = error_and_prediction(w_sgd1, w_sgd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d9ba9f0-c2fa-43c5-9d67-34c2188fdba4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'Stochastic_Gradient_Descent' \n",
    "create_csv_submission(test_ids, y_pred_sgd, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea752dd-9682-4a69-9af5-b85f5c1ed107",
   "metadata": {},
   "source": [
    "F1 score: 0.158\n",
    "\n",
    "Accuracy: 0.558"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6549ae8-fcf1-49ef-b4a4-7043612bd432",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8de75ac1-d4ab-4acb-bb2b-de317832e2d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w_ls1, loss_ls1 = least_squares(y_train[i_1], x_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a9df620-7747-4706-b298-e205b6ab717c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w_ls2, loss_ls2 = least_squares(y_train[i_2], x_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c46868e6-4371-4225-9793-c60ef4031228",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04082041310188043\n",
      "0.028765329027445052\n"
     ]
    }
   ],
   "source": [
    "print(loss_ls1)\n",
    "print(loss_ls2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dcc3b326-836e-4e59-b0c6-c53b70e78bb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mistake percentage on the train set is: 11.1415 %.\n",
      "Our prediction for y_test is: [-1. -1. -1. ... -1.  1.  1.] .\n"
     ]
    }
   ],
   "source": [
    "y_pred_ls = error_and_prediction(w_ls1, w_ls2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01e642b0-7447-4624-9f7a-f48a4db87b9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'Least_Squares' \n",
    "create_csv_submission(test_ids, y_pred_ls, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a543c230-0764-466f-8aaf-5937d4e53957",
   "metadata": {
    "tags": []
   },
   "source": [
    "F1 score: 0.379\n",
    "\n",
    "Accuracy: 0.787"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c81e5c-416f-475d-82a5-3e4ec3232311",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Rigde regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c26b30aa-d527-43a3-85cb-5de32943d3af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parameter: \n",
    "lambda_ = 0.1\n",
    "\n",
    "w_rr1, loss_rr1 = ridge_regression(y_train[i_1], x_train1, lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c51261fa-afcb-42ca-a553-13b2ca10534e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w_rr2, loss_rr2 = ridge_regression(y_train[i_2], x_train2, lambda_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01bc9b8-8108-4ab0-abe7-4f3d81d3cf60",
   "metadata": {},
   "source": [
    "Let us now predict our test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b920893-c4ef-4125-8f59-72ca0367b442",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mistake percentage on the train set is: 10.0166 %.\n",
      "Our prediction for y_test is: [-1. -1. -1. ... -1.  1. -1.] .\n"
     ]
    }
   ],
   "source": [
    "y_pred_rr = error_and_prediction(w_rr1, w_rr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a2b961de-96eb-4ae7-a734-3cadb52eb66a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'Ridge_Regression' \n",
    "create_csv_submission(test_ids, y_pred_rr, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9355b09-385b-48e8-9b69-ab48b2223208",
   "metadata": {},
   "source": [
    "F1 score: 0.380\n",
    "\n",
    "Accuracy: 0.788"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019f1c3a-7ccf-4557-a748-26c7c812c800",
   "metadata": {},
   "source": [
    "### We try now to use Cross-Validation (CV) to see if it changes our accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a16e5a9-d7ea-49bb-87f5-5fc4f855b2dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameter lambda is: 1e-06\n",
      "The best parameter lambda is: 1e-06\n"
     ]
    }
   ],
   "source": [
    "lambdas = np.logspace(-6, 0, 6)\n",
    "k = 4\n",
    "\n",
    "w1, loss1 = ridge_regression_cross_validation(y_train[i_1], x_train1, k, lambdas)\n",
    "w2, loss2 = ridge_regression_cross_validation(y_train[i_2], x_train2, k, lambdas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8407d18e-b326-45d1-832e-068114ba622a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mistake percentage on the train set is: 11.1411 %.\n",
      "Our prediction for y_test is: [-1. -1. -1. ... -1.  1.  1.] .\n"
     ]
    }
   ],
   "source": [
    "y_pred_rr_cv = error_and_prediction(w1, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6ff11d6-dba3-4a05-9acc-096130c18e80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'Ridge_Regression_CV' \n",
    "create_csv_submission(test_ids, y_pred_rr_cv, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8829804e-390f-4ac9-8f7b-9837954b3a31",
   "metadata": {},
   "source": [
    "F1 score: 0.380\n",
    "\n",
    "Accuracy: 0.788"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273163b5-5d1f-471a-bf9b-b6aad89863d2",
   "metadata": {},
   "source": [
    "We see here that using cross validation does not change our accuracy or F1 score in this case. By modifying lambdas or k, these values changes but not much."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c72f0f2-b087-4ac4-9b63-23497d2c327b",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Logistique regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "717dc5ba-f637-4af7-9329-3537ea1d9bbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialization:\n",
    "D1 = x_train1.shape[1]\n",
    "w1 = np.zeros((D1,))\n",
    "\n",
    "# Parameters: \n",
    "gamma = 0.001\n",
    "max_iters = 100\n",
    "\n",
    "# Computation of the weights and loss:\n",
    "w_lr1, loss_lr1 = logistic_regression(y_train[i_1], x_train1, w1, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "03dc51ae-7c93-41b4-ab31-13952da02fd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialization:\n",
    "D2 = x_train2.shape[1]\n",
    "w2 = np.zeros((D2,))\n",
    "\n",
    "# Parameters: \n",
    "gamma = 0.001\n",
    "max_iters = 100\n",
    "\n",
    "# Computation of the weights and loss:\n",
    "w_lr2, loss_lr2 = logistic_regression(y_train[i_2], x_train2, w2, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6f2803d5-1c2c-4a1a-b0e8-a490b20b2d60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6687461415287278\n",
      "0.6705917743935812\n"
     ]
    }
   ],
   "source": [
    "print(loss_lr1)\n",
    "print(loss_lr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f3abbb90-fa25-436c-ba20-9cf7bc71cf76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mistake percentage on the train set is: 8.8205 %.\n",
      "Our prediction for y_test is: [-1. -1. -1. ... -1. -1. -1.] .\n"
     ]
    }
   ],
   "source": [
    "y_pred_lr = error_and_prediction(w_lr1, w_lr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "29c2ec0d-4c3f-4425-8c46-64c3932928cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'Logistic_Regression' \n",
    "create_csv_submission(test_ids, y_pred_lr, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b5d573-7f54-4fa2-b717-6118b21e7a76",
   "metadata": {},
   "source": [
    "F1 score: 0.332\n",
    "\n",
    "Accuracy: 0.903"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9805337a-f0a6-4845-b6f6-2a4e46797e36",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Regularized Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a017a75-b2e6-463a-ba12-e2a95dcc8a6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialization:\n",
    "D1 = x_train1.shape[1]\n",
    "initial_w1 = np.zeros((D1,))\n",
    "\n",
    "# Parameters:\n",
    "lambda_ = 0.01\n",
    "gamma = 0.001\n",
    "max_iters = 1000\n",
    "\n",
    "# Computation of the weights and loss:\n",
    "w_rlr1, loss_rlr1 = reg_logistic_regression(y_train[i_1], x_train1, lambda_, initial_w1, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4c23fb-d00b-4c6a-99b5-19d1f2f347f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialization:\n",
    "D2 = x_train2.shape[1]\n",
    "initial_w2 = np.zeros((D2,))\n",
    "\n",
    "# Parameters:\n",
    "lambda_ = 0.01\n",
    "gamma = 0.001\n",
    "max_iters = 1000\n",
    "\n",
    "# Computation of the weights and loss:\n",
    "w_rlr2, loss_rlr2 = reg_logistic_regression(y_train[i_2], x_train2, lambda_, initial_w1, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8c5702-4e19-4f6b-a8b7-a8d083e2bcce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(loss_rlr1)\n",
    "print(loss_rlr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebd4928-2ec6-4934-9641-577f5742a2af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred_rlr = error_and_prediction(w_rlr1, w_rlr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d584c51-e454-4f64-903e-ee9b8430146b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all(y_pred_rlr==-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d106f190-3886-4efd-85f0-fdcabeafa38d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'Ridge_Logistic_Regression_1000' \n",
    "create_csv_submission(test_ids, y_pred_rlr, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a209a9ca-e757-415a-a1b3-5bb46574c45f",
   "metadata": {},
   "source": [
    "400 iterations:\n",
    "F1 score: 0.385\n",
    "and\n",
    "Accuracy: 0.896\n",
    "\n",
    "1000 iterations:\n",
    "F1 score: 0.369\n",
    "and\n",
    "Accurcy: 0.890\n",
    "\n",
    "\n",
    "#### We see that by increasing the number of iterations, the F1 score increases but accuracy decreases. With 100 iterations, we got a F1 score of 0.012 (not good at all) and an accuracy of 0.912. This means that this algorithm needs more iterations to be performant. Nevertheless, even with 1000 iterations, it takes only a few "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
